<!DOCTYPE html>
<html lang="en" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions sign: 2024-4-28 💡 在看这个文章之前，首先要对 consistency model ，score-based model要有一定了解，以及相应的知识蒸馏，怎样去做单步生成的等等 扩散模型由于其迭代采样过程而导致显著的延迟。为了减轻这些限制，作者引入了一种双重方法，包括 模型小型化 和 减少采样步骤 ，旨在显著降低模型延迟。还是利用知识蒸馏来简化U-Net和图像解码器架构，并引入了一种利用特征匹配和分数蒸馏的创新的单步DM训练技术。 作者提出了两种模型，SDXS-512和SDXS-1024，分别在单个GPU上实现了大约100 FPS(比SD v1.5快30倍)和30 FPS(比SDXL快60倍)的推理速度。此外，我们的训练方法在图像条件控制中提供了有前途的应用，促进了有效的图像到图像的翻译。 1. Introduction 这是一篇小米的工作，实时进行图像到图像的生成。因为是要更多考虑手机的部署，所以要潜在解决的就是模型的大规模和多步骤采样的问题。 首先介绍一下什么是 NFE，即 Number of Function Evaluations，模型评估次数。 对于常见的文生图模型，成本可以近似地计算为总延迟，其中包括文本编码器、图像解码器和去噪模型的延迟，乘以函数评估次数(NFEs)。 这里是直观的一个对比： 本文主要考虑减小 VAE解码器 和 UNet 的规模，两者都是资源密集型组件。作者还是通过蒸馏的 Loss 和 GAN Loss 来轻量化。 🤔这应该是和 stability 的那个 Turbo，UFO-Gen 的原理差不多 另外，为了减少 NFEs, 作者提出了一个快速和稳定的训练方法： 首先，我们建议通过将蒸馏损失函数替换为所提出的特征匹配损失函数，调整采样轨迹并将多步模型快速微调为一步模型。 然后，拓展了 Diff-Instruct 训练策略，利用特征匹配损失的梯度取代分数蒸馏在时间步长的后半部分提供的梯度。 💡 从预训练的扩散模型进行知识蒸馏（分数蒸馏） 都是 来自于 DreamFusion 这篇工作，感兴趣可以看看。 Diff-instruct: A universal approach for transferring knowledge from pre-trained diffusion models.">
<title>SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions</title>

<link rel='canonical' href='https://tlooh.github.io/p/sdxs-real-time-one-step-latent-diffusion-models-with-image-conditions/'>

<link rel="stylesheet" href="/scss/style.min.add8fb3bab9b743b892af012a73b6880459349cb1f020823ea8cdafee6e03d6d.css"><meta property='og:title' content="SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions">
<meta property='og:description' content="SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions sign: 2024-4-28 💡 在看这个文章之前，首先要对 consistency model ，score-based model要有一定了解，以及相应的知识蒸馏，怎样去做单步生成的等等 扩散模型由于其迭代采样过程而导致显著的延迟。为了减轻这些限制，作者引入了一种双重方法，包括 模型小型化 和 减少采样步骤 ，旨在显著降低模型延迟。还是利用知识蒸馏来简化U-Net和图像解码器架构，并引入了一种利用特征匹配和分数蒸馏的创新的单步DM训练技术。 作者提出了两种模型，SDXS-512和SDXS-1024，分别在单个GPU上实现了大约100 FPS(比SD v1.5快30倍)和30 FPS(比SDXL快60倍)的推理速度。此外，我们的训练方法在图像条件控制中提供了有前途的应用，促进了有效的图像到图像的翻译。 1. Introduction 这是一篇小米的工作，实时进行图像到图像的生成。因为是要更多考虑手机的部署，所以要潜在解决的就是模型的大规模和多步骤采样的问题。 首先介绍一下什么是 NFE，即 Number of Function Evaluations，模型评估次数。 对于常见的文生图模型，成本可以近似地计算为总延迟，其中包括文本编码器、图像解码器和去噪模型的延迟，乘以函数评估次数(NFEs)。 这里是直观的一个对比： 本文主要考虑减小 VAE解码器 和 UNet 的规模，两者都是资源密集型组件。作者还是通过蒸馏的 Loss 和 GAN Loss 来轻量化。 🤔这应该是和 stability 的那个 Turbo，UFO-Gen 的原理差不多 另外，为了减少 NFEs, 作者提出了一个快速和稳定的训练方法： 首先，我们建议通过将蒸馏损失函数替换为所提出的特征匹配损失函数，调整采样轨迹并将多步模型快速微调为一步模型。 然后，拓展了 Diff-Instruct 训练策略，利用特征匹配损失的梯度取代分数蒸馏在时间步长的后半部分提供的梯度。 💡 从预训练的扩散模型进行知识蒸馏（分数蒸馏） 都是 来自于 DreamFusion 这篇工作，感兴趣可以看看。 Diff-instruct: A universal approach for transferring knowledge from pre-trained diffusion models.">
<meta property='og:url' content='https://tlooh.github.io/p/sdxs-real-time-one-step-latent-diffusion-models-with-image-conditions/'>
<meta property='og:site_name' content='Tlooh Blog Site'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='Diffusion 加速' /><meta property='article:published_time' content='2024-04-28T01:12:44&#43;08:00'/><meta property='article:modified_time' content='2024-04-28T01:12:44&#43;08:00'/><meta property='og:image' content='https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401795.png' />
<meta name="twitter:title" content="SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions">
<meta name="twitter:description" content="SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions sign: 2024-4-28 💡 在看这个文章之前，首先要对 consistency model ，score-based model要有一定了解，以及相应的知识蒸馏，怎样去做单步生成的等等 扩散模型由于其迭代采样过程而导致显著的延迟。为了减轻这些限制，作者引入了一种双重方法，包括 模型小型化 和 减少采样步骤 ，旨在显著降低模型延迟。还是利用知识蒸馏来简化U-Net和图像解码器架构，并引入了一种利用特征匹配和分数蒸馏的创新的单步DM训练技术。 作者提出了两种模型，SDXS-512和SDXS-1024，分别在单个GPU上实现了大约100 FPS(比SD v1.5快30倍)和30 FPS(比SDXL快60倍)的推理速度。此外，我们的训练方法在图像条件控制中提供了有前途的应用，促进了有效的图像到图像的翻译。 1. Introduction 这是一篇小米的工作，实时进行图像到图像的生成。因为是要更多考虑手机的部署，所以要潜在解决的就是模型的大规模和多步骤采样的问题。 首先介绍一下什么是 NFE，即 Number of Function Evaluations，模型评估次数。 对于常见的文生图模型，成本可以近似地计算为总延迟，其中包括文本编码器、图像解码器和去噪模型的延迟，乘以函数评估次数(NFEs)。 这里是直观的一个对比： 本文主要考虑减小 VAE解码器 和 UNet 的规模，两者都是资源密集型组件。作者还是通过蒸馏的 Loss 和 GAN Loss 来轻量化。 🤔这应该是和 stability 的那个 Turbo，UFO-Gen 的原理差不多 另外，为了减少 NFEs, 作者提出了一个快速和稳定的训练方法： 首先，我们建议通过将蒸馏损失函数替换为所提出的特征匹配损失函数，调整采样轨迹并将多步模型快速微调为一步模型。 然后，拓展了 Diff-Instruct 训练策略，利用特征匹配损失的梯度取代分数蒸馏在时间步长的后半部分提供的梯度。 💡 从预训练的扩散模型进行知识蒸馏（分数蒸馏） 都是 来自于 DreamFusion 这篇工作，感兴趣可以看看。 Diff-instruct: A universal approach for transferring knowledge from pre-trained diffusion models."><meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content='https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401795.png' />
  


    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended">



        <div id="article-toolbar" style="position: sticky;top: 5px;z-index: 1000;">
            <a href="/" class="back-home">
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="15 6 9 12 15 18" />
</svg>



                <span>Back</span>
            </a>
        </div>
    

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">Table of contents</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#1-introduction">1. Introduction</a></li>
    <li><a href="#2-preliiminaries">2. Preliiminaries</a>
      <ol>
        <li><a href="#score-based-model">Score-based Model</a></li>
        <li><a href="#diff-instruct">Diff-Instruct</a></li>
      </ol>
    </li>
    <li><a href="#3-method">3. Method</a>
      <ol>
        <li><a href="#architecture-optimization">Architecture Optimization</a>
          <ol>
            <li><a href="#vae-decoder-distillation">VAE Decoder Distillation</a></li>
            <li><a href="#unet-distillation">UNet Distillation</a></li>
            <li><a href="#controlnet">ControlNet</a></li>
          </ol>
        </li>
        <li><a href="#one-step-training">One-Step Training</a>
          <ol>
            <li><a href="#feature-matching-warmup">Feature Matching Warmup</a></li>
            <li><a href="#segmented-score-distillation">Segmented Score Distillation.</a></li>
            <li><a href="#lora">LoRA</a></li>
            <li><a href="#controlnet-1">ControlNet</a></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#4-experiments">4. Experiments</a></li>
    <li><a href="#5-conclusion">5. Conclusion</a></li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="has-image main-article">
    <header class="article-header">
        <div class="article-image">
            <a href="/p/sdxs-real-time-one-step-latent-diffusion-models-with-image-conditions/">
                
                    <img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401795.png" loading="lazy" alt="Featured image of post SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions" />
                
            </a>
        </div>
    

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/diffusion/" >
                Diffusion
            </a>
        
            <a href="/categories/papers/" >
                Papers
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/sdxs-real-time-one-step-latent-diffusion-models-with-image-conditions/">SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Apr 28, 2024</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    2 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <h1 id="sdxs-real-time-one-step-latent-diffusion-models-with-image-conditions">SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions
</h1><blockquote>
<p><strong>sign: 2024-4-28 💡</strong>
在看这个文章之前，首先要对 consistency model ，score-based model要有一定了解，以及相应的知识蒸馏，怎样去做单步生成的等等</p>
</blockquote>
<blockquote>
<p>扩散模型由于其迭代采样过程而导致显著的延迟。为了减轻这些限制，作者引入了一种双重方法，包括 <font color="#0071e3">模型小型化</font> 和 <font color="#0071e3">减少采样步骤</font> ，旨在显著降低模型延迟。还是利用知识蒸馏来简化U-Net和图像解码器架构，并引入了一种利用特征匹配和分数蒸馏的创新的单步DM训练技术。</p>
<p>作者提出了两种模型，<strong>SDXS-512</strong>和<strong>SDXS-1024</strong>，分别在单个GPU上实现了大约100 FPS(比SD v1.5快30倍)和30 FPS(比SDXL快60倍)的推理速度。此外，我们的训练方法在图像条件控制中提供了有前途的应用，促进了有效的图像到图像的翻译。</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401795.png"
	
	
	
	loading="lazy"
	
		alt="alt text"
	
	
></p>
<h2 id="1-introduction">1. Introduction
</h2><p>这是一篇小米的工作，实时进行图像到图像的生成。因为是要更多考虑手机的部署，所以要潜在解决的就是模型的大规模和多步骤采样的问题。</p>
<blockquote>
<p>首先介绍一下什么是 <code>NFE</code>，即 <strong>Number of Function Evaluations</strong>，模型评估次数。</p>
<p>对于常见的文生图模型，成本可以近似地计算为总延迟，其中包括文本编码器、图像解码器和去噪模型的延迟，乘以函数评估次数(NFEs)。</p>
</blockquote>
<p>这里是直观的一个对比：</p>
<p><img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401816.png"
	
	
	
	loading="lazy"
	
		alt="alt text"
	
	
></p>
<p>本文主要考虑减小 VAE解码器 和 UNet 的规模，两者都是资源密集型组件。作者还是通过蒸馏的 Loss 和 GAN Loss 来轻量化。</p>
<blockquote>
<p>🤔这应该是和 stability 的那个 Turbo，UFO-Gen 的原理差不多</p>
</blockquote>
<p>另外，为了减少 NFEs, 作者提出了一个快速和稳定的训练方法：</p>
<ul>
<li>首先，我们建议通过将蒸馏损失函数替换为所提出的特征匹配损失函数，调整采样轨迹并将多步模型快速微调为一步模型。</li>
<li>然后，拓展了 Diff-Instruct 训练策略，利用特征匹配损失的梯度取代分数蒸馏在时间步长的后半部分提供的梯度。</li>
</ul>
<blockquote>
<p><font color="#fc5531"><b>💡 从预训练的扩散模型进行知识蒸馏（分数蒸馏）</b></font> 都是 来自于 <strong>DreamFusion</strong> 这篇工作，感兴趣可以看看。
Diff-instruct: A universal approach for transferring knowledge from pre-trained diffusion models.</p>
</blockquote>
<h2 id="2-preliiminaries">2. Preliiminaries
</h2><h3 id="score-based-model">Score-based Model
</h3><p>所使用的损失函数的核心是分数匹配(SM)损失，其目的是使模型估计的分数与数据的真实分数之间的差异最小化:</p>
<p><img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401825.png"
	
	
	
	loading="lazy"
	
		alt="alt text"
	
	
></p>
<h3 id="diff-instruct">Diff-Instruct
</h3><p>Diff-Instruct 通过 Integral Kullback-Leibler (IKL)散度，将分数蒸馏引入到图像的生成过程，扩散模型其实实在拟合 $p, q$两个分布之间的差异。这里我们可以定义优化目标为：</p>
<p><img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401834.png"
	
	
	
	loading="lazy"
	
		alt="alt text"
	
	
></p>
<p>其中 $q_t$ 和 $p_t$ 代表 $t$ 时刻扩散过程的边缘概率密度，$q_0$ 和 $p_0$ 之间的 IKL 梯度可以表示为：</p>
<p><img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401843.png"
	
	
	
	loading="lazy"
	
		alt="alt text"
	
	
></p>
<p>其中 $\boldsymbol{x}<em>0=g</em>\theta(\boldsymbol{z})$ 表示随机采样的高斯噪声 $z$ 通过单步生成的 $x_0$. $\boldsymbol{s}<em>\phi$ 和 $s</em>{p_t}$ 分别表示在线训练 Diffusion Model 和预训练 Diffusion Model的分数函数。</p>
<p>Diff-Instruct 直接使用这个梯度来更新模型，当两个分数函数的输出一致时，单步步生成器输出的边缘分布与预训练DM的边缘分布一致。</p>
<h2 id="3-method">3. Method
</h2><p><img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401854.png"
	
	
	
	loading="lazy"
	
		alt="alt text"
	
	
></p>
<h3 id="architecture-optimization">Architecture Optimization
</h3><h4 id="vae-decoder-distillation">VAE Decoder Distillation
</h4><p>原始的 <strong>VAE是通过平衡三种损失来优化的:重建、Kullback-Leibler (KL)散度和GAN损失</strong>。然而，在训练过程中平等地对待所有样本会引入冗余。利用预训练的扩散模型  $F$ 对隐编码 $z$ 进行采样，并利用预训练的VAE解码器对图像 $\tilde{\boldsymbol{x}}$ 进行重构，我们引入了VAE蒸馏(VD)损失来训练一个微小的图像解码器G:</p>
<p><img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401873.png"
	
	
	
	loading="lazy"
	
		alt="alt text"
	
	
></p>
<p>$D$ 是 GAN 的判别器，$\lambda_{GAN}$ 用来平衡两个损失项， $|G(\boldsymbol{z})<em>{\downarrow8\times}-\tilde{\boldsymbol{x}}</em>{\downarrow8\times}|_1$ 通过 8x 下采样的图像计算的 L1 损失来保持图像的细节。</p>
<h4 id="unet-distillation">UNet Distillation
</h4><p>受启发于 <a class="link" href="https://arxiv.org/abs/2305.15798"  target="_blank" rel="noopener"
    >BK-sdm</a> 的知识蒸馏策略，即有选择性地从U-Net中去除残余和Transformer块，旨在训练一个更紧凑的模型，该模型仍然可以有效地再现原始模型的中间特征映射和输出。图2 (b)说明了提取微小U-Net的训练策略。知识蒸馏通过输出知识蒸馏(OKD)和特征知识蒸馏(FKD)损失实现:</p>
<p><img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401887.png"
	
	
	
	loading="lazy"
	
		alt="alt text"
	
	
></p>
<p>总体损失是两个损失函数的结合：</p>
<p><img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401896.png"
	
	
	
	loading="lazy"
	
		alt="alt text"
	
	
></p>
<h4 id="controlnet">ControlNet
</h4><p>这里提一下 ControlNet。 ControlNet通过在现有的文本到图像框架中嵌入空间引导来增强扩散模型，从而实现图像到图像的任务，如素描到图像的翻译、绘画和超分辨率。它复制了U-Net的编码器架构和参数，增加了额外的卷积层来整合空间控制。尽管继承了U-Net的参数并使用零卷积来增强训练稳定性，<strong>ControlNet的训练过程仍然昂贵，并且受到数据集质量的显著影响。</strong></p>
<p>为了解决这些挑战，我们提出了一种蒸馏方法，将原始U-Net的控制网提炼成相应的微型U-Net的控制网。如图2 (b)所示，我们不是直接提取ControlNet的零卷积输出，而是将ControlNet与U-Net结合起来，然后提取U-Net的中间特征映射和输出，这使得蒸馏后的ControlNet和微小的U-Net能够更好地协同工作。考虑到ControlNet不影响U-Net编码器的特征映射，特征蒸馏只应用于U-Net的解码器</p>
<h3 id="one-step-training">One-Step Training
</h3><p>虽然dm在图像生成方面表现出色，但即使使用先进的采样器，它们对多个采样步骤的依赖也会带来显著的推理延迟为了解决这个问题，之前的研究引入了知识蒸馏技术，<strong>如渐进式蒸馏和一致性蒸馏</strong>，旨在减少采样步骤并加速推理。然而，这些方法通常只能通过4 ~ 8个采样步骤产生清晰的图像，这与GAN中的一步生成过程形成鲜明对比。</p>
<p>探索将GAN整合到DM训练体系中已经显示出增强图像质量的希望。然而，GAN也有自己的挑战，包括对超参数的敏感性和训练不稳定性。对于一步生成模型，有必要寻求一种更稳定的训练策略。</p>
<h4 id="feature-matching-warmup">Feature Matching Warmup
</h4><p>通常做法是是训练一个单步模型，即：初始采样一个噪声 $\epsilon$，然后使用常微分方程求解器 $\psi$ 来进行采样最终得到生成图像 $\boldsymbol{\hat{x}}_{0}$。</p>
<p>然而，这种方法经常导致产生低质量的图像。潜在的问题是使用来自预训练DM的ODE采样器生成的噪声图像对的采样轨迹中的交叉，导致不适定问题。整流通过调整采样轨迹解决了这一挑战。它取代了训练目标，并提出了一个“回流”策略来完善配对从而最小化轨迹交叉。相反，我们注意到采样轨迹的交叉会导致一个噪声输入对应于多个真地图像，导致训练模型生成的图像是多个可行输出的加权和，权重为$w(y)$:</p>
<p><img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401908.png"
	
	
	
	loading="lazy"
	
		alt="alt text"
	
	
></p>
<p>对于最常用的均方误差(mean square error, MSE)损失，<strong>该模型在包含多个可行目标的情况下，倾向于输出多个可行解的平均值，以使整体误差最小化，从而导致生成的图像模糊。</strong> 为了解决这个问题，我们探索了替代损失函数，改变加权方案，优先考虑更清晰的图像。在大多数情况下，我们可以使用L1损失、感知损失和LPIPS损失来改变权重的形式。我们建立在特征匹配方法的基础上，该方法涉及计算编码器模型生成的中间特征映射上的损失。具体来说，我们从dist损失中获得灵感，在这些特征映射上应用结构相似指数(SSIM)，以获得更精细的特征匹配损失:</p>
<p><img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401918.png"
	
	
	
	loading="lazy"
	
		alt="alt text"
	
	
></p>
<p>其中 $w_l$ 是 SSIM 损失的权重，通过编码器 $\boldsymbol{f_\theta}$ 编码的第 <code>l-th</code> 中间特征上计算得到的 SSIM 损失，$\boldsymbol{x_\theta(\epsilon)}$ 是由微型的 U-Net $\boldsymbol{x_\theta}$ 生成的图像，$\boldsymbol{\psi(x_\phi(\epsilon))}$ 是由原始的 U-Net $\boldsymbol{x_\phi}$ 用ODE采样器 $\boldsymbol{\psi}$ 生成的图像。在实践中，我们发现使用预训练的CNN骨干网、ViT骨干网和DM U-Net编码器都能获得良好的效果，与MSE损失的对比如图6所示。此外，我们还利用现有的微调方法(如LCM)拉直模型的轨迹以缩小可行输出的范围，或者直接使用公开可用的少步模型。我们将单独使用 $L_{FM}$来训练一步模型作为热身，只依赖少量的训练步骤。</p>
<h4 id="segmented-score-distillation">Segmented Score Distillation.
</h4><p>虽然特征匹配损失可以产生几乎清晰的图像，但不能达到真正的分布匹配，所以训练好的模型只能作为正式训练的初始化。为了解决这一差距，我们详细阐述了diffi - directive中使用的训练策略，该策略旨在通过匹配时间步长的边际分数函数，使模型的输出分布与预训练模型的输出分布更接近。然而，由于需要在 $t \to T$处加入高水平的噪声才能计算目标分数，因此此时估计的分数函数是不准确的。我们注意到扩散模型的采样轨迹从粗到细，这意味着 $t \to t$，分数函数提供低频信息的梯度，而 $t \to 0$，它提供高频信息的梯度。因此，将时间步长分为 $[0, \alpha T]$ 和 $(\alpha T, T]$两段，后者用 $L_{FM}$ 代替，因为它能提供足够的低频梯度。这一策略可以正式表示为:</p>
<p><img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401929.png"
	
	
	
	loading="lazy"
	
		alt="alt text"
	
	
></p>
<p>$\lambda_{FM}$ 用于平衡两段的梯度，$\alpha \in [0, 1]$。我们有意将 $\alpha$ 设置为接近1，$\lambda_{FM}$ 设置为较高值，以确保模型的输出分布平滑地与预训练分数函数预测的分布对齐。在概率密度达到显著重叠后，我们逐渐降低 $\alpha$ 和 $\lambda_{FM}$。图3直观地描述了我们的训练策略，其中离线DM表示预训练DM的U-Net，在线DM从离线DM初始化，并通过Eq.(1)对生成的图像进行微调。在实践中，在线DM和学生DM交替训练，如算法1所示。</p>
<p><img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401939.png"
	
	
	
	loading="lazy"
	
		alt="alt text"
	
	
></p>
<h4 id="lora">LoRA
</h4><p><img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401950.png"
	
	
	
	loading="lazy"
	
		alt="alt text"
	
	
></p>
<p>一旦训练了一步DM，就可以像其他DM一样对其进行微调，以调整生成图像的样式。我们将LoRA与提出的分段分数蒸馏(Segmented Score Distillation)结合起来，对一步DM进行微调，如图4所示。</p>
<p>具体来说，我们将预训练的LoRA插入到离线DM中，如果它也与教师DM兼容，它也被插入到离线DM中。值得注意的是，我们没有将LoRA插入到在线DM中，因为它对应于一步DM的输出分布。然后，我们使用与一步训练相同的训练过程，但跳过特征匹配预热，因为LoRA微调比完全微调稳定得多。此外，当Teacher DM不能包含预训练的LoRA时，我们使用简化的λF M。通过这种方式，可以将预训练的LoRA提取到SDXS的LoRA中。</p>
<h4 id="controlnet-1">ControlNet
</h4><p>我们的方法也可以用于训练ControlNet，使微小的一步模型能够将图像条件合并到其图像生成过程中，如图5所示。与文本到图像生成的基本模型相比，这里训练的模型是蒸馏的ControlNet，伴随着前面提到的微小的U-Net, U-Net的参数在训练过程中是固定的。重要的是，我们需要从教师模型采样的图像中提取控制图像，而不是从数据集图像中提取，以确保噪声、目标图像和控制图像形成配对三元组。此外，原始的多步U-Net附带的预训练控制网与在线U-Net和离线U-Net集成，但不参与培训。与文本编码器类似，该函数仅限于作为预训练的特征提取器。这样，为了进一步降低L，训练后的ControlNet就是学习利用从目标图像中提取的控制图像。同时，分数蒸馏鼓励模型匹配边缘分布，增强生成图像的上下文相关性。值得注意的是，我们发现用新初始化的噪声替换U-Net的一部分噪声输入可以增强控制能力。</p>
<p><img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401962.png"
	
	
	
	loading="lazy"
	
		alt="alt text"
	
	
></p>
<h2 id="4-experiments">4. Experiments
</h2><p>实现细节。我们的代码是基于diffusers库开发的。由于我们无法访问SD v2.1 base和SDXL的训练数据集，因此整个训练过程几乎是无数据的，<strong>仅依赖于可公开访问的数据集中的提示（Laion-5B）</strong>。必要时，我们使用开源预训练模型与这些提示相结合来生成相应的图像。为了训练我们的模型，我们将训练小批大小配置为1024到2048。为了在可用硬件上容纳这个批处理大小，<strong>我们在必要时战略性地实现梯度累积</strong>。值得注意的是，我们发现所提出的训练策略导致模型生成的图像纹理较少。因此，在训练之后，**我们利用GAN损失和极低秩的LoRA进行短时间的微调。**当需要GAN损失时，<strong>我们使用StyleGAN-T中的投影GAN损失</strong>，基本设置与ADD一致。对于SDXS-1024的训练，我们使用SDXL的精简版Vega作为在线DM和离线DM的初始化，以减少训练开销。</p>
<p><img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401971.png"
	
	
	
	loading="lazy"
	
		alt="alt text"
	
	
></p>
<p><img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401981.png"
	
	
	
	loading="lazy"
	
		alt="alt text"
	
	
></p>
<p><img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401992.png"
	
	
	
	loading="lazy"
	
		alt="alt text"
	
	
></p>
<p><img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401002.png"
	
	
	
	loading="lazy"
	
		alt="alt text"
	
	
></p>
<h2 id="5-conclusion">5. Conclusion
</h2><p>本文探讨了基于大规模扩散的文本到图像生成模型的升华，使其能够在gpu上进行实时推理。首先，我们使用知识蒸馏来压缩U-Net架构和图像解码器。随后，我们引入了一种新的训练策略，利用特征匹配和分数蒸馏将采样过程减少到一步。这种方法允许在单个GPU上实时生成1024×1024图像，保持与原始模型相当的质量。此外，我们提出的训练方法也可以适应涉及图像条件生成的任务，避免了预训练的ControlNet的直接适应。我们相信，在边缘设备上部署高效的图像条件生成代表了未来研究的一个有希望的途径，并计划探索其他应用，如喷漆和超分辨率。</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/diffusion-%E5%8A%A0%E9%80%9F/">Diffusion 加速</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.body, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>
    
</article>

    

    

<aside class="related-content--wrapper">
    <h2 class="section-title">Related content</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="has-image">
    <a href="/p/cross-attention-makes-inference-cumbersome-in-text-to-image-diffusion-models/">
        
        
            <div class="article-image">
                
                    <img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202405061001750.png" loading="lazy" data-key="" data-hash="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202405061001750.png"/>
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models</h2>
        </div>
    </a>
</article>

            
                
<article class="has-image">
    <a href="/p/faster-diffusion-rethinking-the-role-of-unet-encoder-in-diffusion-models/">
        
        
            <div class="article-image">
                <img src="/p/faster-diffusion-rethinking-the-role-of-unet-encoder-in-diffusion-models/assets/Faster%20Diffusion/image-7.39bc41021047cf57f10a328634f07859_hube457628ccb7476358da95c276ecf5b4_788663_250x150_fill_box_smart1_3.png" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models"
                        
                        data-hash="md5-ObxBAhBHz1fxCjKGNPB4WQ==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models</h2>
        </div>
    </a>
</article>

            
                
<article class="has-image">
    <a href="/p/adadiff-adaptive-step-selection-for-fast-diffusion/">
        
        
            <div class="article-image">
                
                    <img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/2024/202408010059694.png" loading="lazy" data-key="" data-hash="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/2024/202408010059694.png"/>
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">AdaDiff: Adaptive Step Selection for Fast Diffusion</h2>
        </div>
    </a>
</article>

            
                
<article class="has-image">
    <a href="/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/">
        
        
            <div class="article-image">
                
                    <img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202405061001750.png" loading="lazy" data-key="" data-hash="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202405061001750.png"/>
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">StoryDiffusion：让 Diffusion 用来生成故事插画和视频</h2>
        </div>
    </a>
</article>

            
                
<article class="has-image">
    <a href="/p/all-papers-about-diffusion-models-on-arxiv/">
        
        
            <div class="article-image">
                <img src="/p/all-papers-about-diffusion-models-on-arxiv/_hu97d061de95afac90ca03bfd8d919a16b_239242_538f2624a81473eb1fd40013472e5733.jpeg" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post All papers about diffusion models on arxiv"
                        
                        data-hash="md5-ho7mJyGJc4P2VN3XPYZazA==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">All papers about diffusion models on arxiv</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    <div class="disqus-container">
    <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "hugo-theme-stack" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

<style>
    .disqus-container {
        background-color: var(--card-background);
        border-radius: var(--card-border-radius);
        box-shadow: var(--shadow-l1);
        padding: var(--card-padding);
    }
</style>

<script>
    window.addEventListener('onColorSchemeChange', (e) => {
        if (typeof DISQUS == 'object') {
            DISQUS.reset({
                reload: true
            });
        }
    })
</script>

    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2020 - 
        
        2024 Tlooh
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.26.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
