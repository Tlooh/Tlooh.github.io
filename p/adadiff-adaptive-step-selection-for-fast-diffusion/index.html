<!DOCTYPE html>
<html lang="en" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="AdaDiff: Adaptive Step Selection for Fast Diffusion 这本质是一篇加速生成的文章，优化角度在于为不同复杂程度的提示选取不同的时间步去进行去噪。 Abstract 扩散模型的生成过程涉及到几十个步骤的去噪以产生逼真的图像/视频，这在计算上是昂贵的。本文认为去噪步骤 应该根据输入样本的复杂程度来进行特定的采样。 因此提出了 AdaDiff ，一个轻量级的框架来学习 instance-specific 的步骤策略。还是和之前方法一样，使用策略梯度方法来最大化一个奖励函数。（平衡推理时间和生成质量） 作者在 3 个图像生成基准测试和 2 两个视频生成基准测试进行了实验，结果表明：可以实现和固定 50 个去噪步骤的模型实现相当的结果，但是本文提出的方法减少了 $33% - 40%$ 的推理时间 Introduction 目前考虑到生成质量和推理速度之间的权衡，大多数去噪步骤都是设置为 50。 而作者认为： 文本提示的丰富性(即对象的数量以及它们彼此之间的关系)差异很大。 对于某些简单且粗粒度的提示(仅涉及一个或几个对象)，使用少步骤足以达到好的生成质量，而增加步骤只会有边际改善而不会有必然的生成改善。 对于包含许多对象、详细描述和对象之间复杂交互的复杂文本提示，需要更多的步骤来实现期望的结果。 因此，本文的目标是为扩散模式建立一个动态框架； 简而言之，AdaDiff 为不同丰富程度的提示分配了不同数量的生成步骤，旨在最大限度地减少推理时间，同时保持高图像质量。带有红色边框的图像是由AdaDiff生成的。 对于每个提示符的的动态生成策略： 1）决定所需要的生成步骤数目； 2）使用相对更少的步骤来确保高质量的生成； 这样的问题是：学习动态步骤选择是一项非常难的任务，因为它涉及不可微分的决策过程。 为此，AdaDiff训练一个轻量级的步长选择网络来生成步长使用策略。随后，基于此导出的策略，对预训练的扩散模型进行动态采样过程，以实现高效生成。 步骤选择网络使用策略梯度方法进行优化，以最大化奖励函数。 这个奖励函数的主要目标是鼓励生成高质量的视觉内容，同时最小化计算资源。 同样值得指出的是，以文本输入为条件的步长选择网络是轻量级的，计算开销可以忽略不计。 Methods 这里主要关注的是 3.2 节：Adaptive step selection for image generation Adaptive step selection for image generation 文中使用离散时间的 DDIM sample 来加速采样，设定 $N = 5, S = {10, 20, 30, 40, 50}$ ；">
<title>AdaDiff: Adaptive Step Selection for Fast Diffusion</title>

<link rel='canonical' href='https://tlooh.github.io/p/adadiff-adaptive-step-selection-for-fast-diffusion/'>

<link rel="stylesheet" href="/scss/style.min.add8fb3bab9b743b892af012a73b6880459349cb1f020823ea8cdafee6e03d6d.css"><meta property='og:title' content="AdaDiff: Adaptive Step Selection for Fast Diffusion">
<meta property='og:description' content="AdaDiff: Adaptive Step Selection for Fast Diffusion 这本质是一篇加速生成的文章，优化角度在于为不同复杂程度的提示选取不同的时间步去进行去噪。 Abstract 扩散模型的生成过程涉及到几十个步骤的去噪以产生逼真的图像/视频，这在计算上是昂贵的。本文认为去噪步骤 应该根据输入样本的复杂程度来进行特定的采样。 因此提出了 AdaDiff ，一个轻量级的框架来学习 instance-specific 的步骤策略。还是和之前方法一样，使用策略梯度方法来最大化一个奖励函数。（平衡推理时间和生成质量） 作者在 3 个图像生成基准测试和 2 两个视频生成基准测试进行了实验，结果表明：可以实现和固定 50 个去噪步骤的模型实现相当的结果，但是本文提出的方法减少了 $33% - 40%$ 的推理时间 Introduction 目前考虑到生成质量和推理速度之间的权衡，大多数去噪步骤都是设置为 50。 而作者认为： 文本提示的丰富性(即对象的数量以及它们彼此之间的关系)差异很大。 对于某些简单且粗粒度的提示(仅涉及一个或几个对象)，使用少步骤足以达到好的生成质量，而增加步骤只会有边际改善而不会有必然的生成改善。 对于包含许多对象、详细描述和对象之间复杂交互的复杂文本提示，需要更多的步骤来实现期望的结果。 因此，本文的目标是为扩散模式建立一个动态框架； 简而言之，AdaDiff 为不同丰富程度的提示分配了不同数量的生成步骤，旨在最大限度地减少推理时间，同时保持高图像质量。带有红色边框的图像是由AdaDiff生成的。 对于每个提示符的的动态生成策略： 1）决定所需要的生成步骤数目； 2）使用相对更少的步骤来确保高质量的生成； 这样的问题是：学习动态步骤选择是一项非常难的任务，因为它涉及不可微分的决策过程。 为此，AdaDiff训练一个轻量级的步长选择网络来生成步长使用策略。随后，基于此导出的策略，对预训练的扩散模型进行动态采样过程，以实现高效生成。 步骤选择网络使用策略梯度方法进行优化，以最大化奖励函数。 这个奖励函数的主要目标是鼓励生成高质量的视觉内容，同时最小化计算资源。 同样值得指出的是，以文本输入为条件的步长选择网络是轻量级的，计算开销可以忽略不计。 Methods 这里主要关注的是 3.2 节：Adaptive step selection for image generation Adaptive step selection for image generation 文中使用离散时间的 DDIM sample 来加速采样，设定 $N = 5, S = {10, 20, 30, 40, 50}$ ；">
<meta property='og:url' content='https://tlooh.github.io/p/adadiff-adaptive-step-selection-for-fast-diffusion/'>
<meta property='og:site_name' content='Tlooh Blog Site'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='Diffusion 加速' /><meta property='article:published_time' content='2023-12-21T01:12:44&#43;08:00'/><meta property='article:modified_time' content='2024-08-01T01:18:14&#43;08:00'/><meta property='og:image' content='https://raw.githubusercontent.com/Tlooh/blog-imgs/main/2024/202408010059694.png' />
<meta name="twitter:title" content="AdaDiff: Adaptive Step Selection for Fast Diffusion">
<meta name="twitter:description" content="AdaDiff: Adaptive Step Selection for Fast Diffusion 这本质是一篇加速生成的文章，优化角度在于为不同复杂程度的提示选取不同的时间步去进行去噪。 Abstract 扩散模型的生成过程涉及到几十个步骤的去噪以产生逼真的图像/视频，这在计算上是昂贵的。本文认为去噪步骤 应该根据输入样本的复杂程度来进行特定的采样。 因此提出了 AdaDiff ，一个轻量级的框架来学习 instance-specific 的步骤策略。还是和之前方法一样，使用策略梯度方法来最大化一个奖励函数。（平衡推理时间和生成质量） 作者在 3 个图像生成基准测试和 2 两个视频生成基准测试进行了实验，结果表明：可以实现和固定 50 个去噪步骤的模型实现相当的结果，但是本文提出的方法减少了 $33% - 40%$ 的推理时间 Introduction 目前考虑到生成质量和推理速度之间的权衡，大多数去噪步骤都是设置为 50。 而作者认为： 文本提示的丰富性(即对象的数量以及它们彼此之间的关系)差异很大。 对于某些简单且粗粒度的提示(仅涉及一个或几个对象)，使用少步骤足以达到好的生成质量，而增加步骤只会有边际改善而不会有必然的生成改善。 对于包含许多对象、详细描述和对象之间复杂交互的复杂文本提示，需要更多的步骤来实现期望的结果。 因此，本文的目标是为扩散模式建立一个动态框架； 简而言之，AdaDiff 为不同丰富程度的提示分配了不同数量的生成步骤，旨在最大限度地减少推理时间，同时保持高图像质量。带有红色边框的图像是由AdaDiff生成的。 对于每个提示符的的动态生成策略： 1）决定所需要的生成步骤数目； 2）使用相对更少的步骤来确保高质量的生成； 这样的问题是：学习动态步骤选择是一项非常难的任务，因为它涉及不可微分的决策过程。 为此，AdaDiff训练一个轻量级的步长选择网络来生成步长使用策略。随后，基于此导出的策略，对预训练的扩散模型进行动态采样过程，以实现高效生成。 步骤选择网络使用策略梯度方法进行优化，以最大化奖励函数。 这个奖励函数的主要目标是鼓励生成高质量的视觉内容，同时最小化计算资源。 同样值得指出的是，以文本输入为条件的步长选择网络是轻量级的，计算开销可以忽略不计。 Methods 这里主要关注的是 3.2 节：Adaptive step selection for image generation Adaptive step selection for image generation 文中使用离散时间的 DDIM sample 来加速采样，设定 $N = 5, S = {10, 20, 30, 40, 50}$ ；"><meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content='https://raw.githubusercontent.com/Tlooh/blog-imgs/main/2024/202408010059694.png' />
  


    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended">



        <div id="article-toolbar" style="position: sticky;top: 5px;z-index: 1000;">
            <a href="/" class="back-home">
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="15 6 9 12 15 18" />
</svg>



                <span>Back</span>
            </a>
        </div>
    

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">Table of contents</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#adadiff-adaptive-step-selection-for-fast-diffusion">AdaDiff: Adaptive Step Selection for Fast Diffusion</a></li>
  </ol>

  <ol>
    <li>
      <ol>
        <li>
          <ol>
            <li><a href="#adaptive-step-selection-for-image-generation">Adaptive step selection for image generation</a></li>
            <li><a href="#reward-function">Reward function</a></li>
            <li><a href="#training-and-inference">Training and inference</a></li>
          </ol>
        </li>
      </ol>
    </li>
  </ol>

  <ol>
    <li>
      <ol>
        <li>
          <ol>
            <li><a href="#metric">Metric</a></li>
            <li><a href="#results">Results</a></li>
          </ol>
        </li>
      </ol>
    </li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="has-image main-article">
    <header class="article-header">
        <div class="article-image">
            <a href="/p/adadiff-adaptive-step-selection-for-fast-diffusion/">
                
                    <img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/2024/202408010059694.png" loading="lazy" alt="Featured image of post AdaDiff: Adaptive Step Selection for Fast Diffusion" />
                
            </a>
        </div>
    

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/diffusion/" >
                Diffusion
            </a>
        
            <a href="/categories/papers/" >
                Papers
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/adadiff-adaptive-step-selection-for-fast-diffusion/">AdaDiff: Adaptive Step Selection for Fast Diffusion</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Dec 21, 2023</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    2 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <h2 id="adadiff-adaptive-step-selection-for-fast-diffusion">AdaDiff: Adaptive Step Selection for Fast Diffusion
</h2><blockquote>
<p>这本质是一篇加速生成的文章，优化角度在于为不同复杂程度的提示选取不同的时间步去进行去噪。</p>
</blockquote>
<h1 id="abstract">Abstract
</h1><p>扩散模型的生成过程涉及到几十个步骤的去噪以产生逼真的图像/视频，这在计算上是昂贵的。本文认为去噪步骤 <strong><font color="#00b0f0">应该根据输入样本的复杂程度来进行特定的采样</font></strong>。</p>
<p>因此提出了 <strong>AdaDiff</strong> ，一个轻量级的框架来学习 <code>instance-specific</code>  的步骤策略。还是和之前方法一样，使用策略梯度方法来最大化一个奖励函数。（平衡推理时间和生成质量）</p>
<p>作者在 3 个图像生成基准测试和 2 两个视频生成基准测试进行了实验，结果表明：可以实现和固定 50 个去噪步骤的模型实现相当的结果，但是本文提出的方法减少了 $33% - 40%$ 的推理时间</p>
<hr>
<h1 id="introduction">Introduction
</h1><p><img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/2024/202408010058261.png"
	
	
	
	loading="lazy"
	
		alt="A conceptual overview of our approach."
	
	
></p>
<p>目前考虑到生成质量和推理速度之间的权衡，大多数去噪步骤都是设置为 50。
而作者认为：</p>
<ul>
<li>文本提示的丰富性(即对象的数量以及它们彼此之间的关系)差异很大。</li>
<li>对于某些简单且粗粒度的提示(仅涉及一个或几个对象)，使用少步骤足以达到好的生成质量，而增加步骤只会有边际改善而不会有必然的生成改善。</li>
<li>对于包含许多对象、详细描述和对象之间复杂交互的复杂文本提示，需要更多的步骤来实现期望的结果。
<font color="#d83931"><b>因此，本文的目标是为扩散模式建立一个动态框架；</b></font></li>
</ul>
<p>简而言之，AdaDiff 为不同丰富程度的提示分配了不同数量的生成步骤，旨在最大限度地减少推理时间，同时保持高图像质量。带有红色边框的图像是由AdaDiff生成的。</p>
<p>对于每个提示符的的动态生成策略：
1）决定所需要的生成步骤数目；
2）使用相对更少的步骤来确保高质量的生成；</p>
<p><strong>这样的问题是：<font color="#d83931"><b>学习动态步骤选择是一项非常难的任务，因为它涉及不可微分的决策过程。</b></font></strong>
为此，AdaDiff训练一个轻量级的步长选择网络来生成步长使用策略。随后，基于此导出的策略，对预训练的扩散模型进行动态采样过程，以实现高效生成。</p>
<ul>
<li>步骤选择网络使用策略梯度方法进行优化，以最大化奖励函数。</li>
<li>这个奖励函数的主要目标是鼓励生成高质量的视觉内容，同时最小化计算资源。</li>
<li>同样值得指出的是，以文本输入为条件的步长选择<strong>网络是轻量级的，计算开销可以忽略不计。</strong></li>
</ul>
<hr>
<h1 id="methods">Methods
</h1><blockquote>
<p>这里主要关注的是 3.2 节：Adaptive step selection for image generation</p>
</blockquote>
<h4 id="adaptive-step-selection-for-image-generation">Adaptive step selection for image generation
</h4><p>文中使用离散时间的 DDIM sample 来加速采样，设定 $N = 5, S = {10, 20, 30, 40, 50}$ ；</p>
<p>形式上;</p>
<p>1）给定文本提示 $\mathbf{p}$  ，经过 文本编码器 $\boldsymbol{\tau}$ 提取特征 $\mathbf{c}=\boldsymbol{\tau}(\mathbf{p})$
2）步长选择网络 $f_{s}$，参数化为 $w$，通过 <code>self-attention</code> 来学习文本提示 $\mathbf{c}$ 信息量；
3）然后通过一个 MLP 投射为 $s\in \mathcal{\mathbb{R}}^{N}$</p>
<p>$$
\mathbf{s}=f_s(\mathbf{c};\mathbf{w})
$$</p>
<p>其中，$\mathbf{s}$ 中的每一项表示选择该步骤的<strong>概率得分</strong>。然后定义了一个N维类别分布的步长选择策略 $\pi^f\left(\mathbf{u}\mid\mathbf{p}\right)$。</p>
<p>其中，$\mathbf{u}$ 是一个长度为 N 的 one-hot 向量，而 $\mathbf{u}_j = 1$ 表示 $\mathcal{S}$ 中选择下标为 $j$ 的时间步骤 $\mathbf{t}$ 的概率。</p>
<blockquote>
<p>步长选择网络设计为由三个自关注层和一个多层感知器组成的轻量级结构。</p>
</blockquote>
<p>在训练阶段，从相应的策略中抽样生成 $\mathbf{u}$，在测试阶段，采用贪心方法。</p>
<p><img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/2024/202408010059694.png"
	
	
	
	loading="lazy"
	
		alt="An overview of the proposed pipeline AdaDiff"
	
	
></p>
<h4 id="reward-function">Reward function
</h4><p>两部分：1）Image quality Reward ， 2）step reward (balance quality 和 inference time)</p>
<h6 id="image-quality-reward">Image quality Reward
</h6><p>这一部分是使用了一个图像质量评估模型 $f_q$，表示为 <strong>IQS</strong>。
模型从两个维度评估图像质量：</p>
<ul>
<li><strong><font color="#d83931">image-text alignment</font></strong>：图文对齐来衡量生成图像和文本提示之间的一致性。</li>
<li><strong><font color="#d83931">perceptual fidelity</font></strong>：生成的图像应该忠实于物体的形状和特征，而不是杂乱无章地生成。</li>
</ul>
<p>因此，IQS score 的分数反映着图像的质量。<strong>image quality reward fomulate as：</strong> $\mathcal{Q}(\mathbf{u})=f_q(\mathbf{x})$</p>
<h6 id="step-reward">Step reward
</h6><p>$$\mathcal{O}(\mathbf{u})=1-\frac{\mathbf{t}}{S_{max}}$$</p>
<p>表示为相对于 $\mathcal{S}$ 中的最大时间步节省的规范化步骤。（相当于省下了多少时间）</p>
<p>整体的奖励函数如下所示：</p>
<p>$$
\left.R(\mathbf{u})=\left{\begin{array}{ll}\mathcal{O}(\mathbf{u})+\lambda\mathcal{Q}(\mathbf{u})&amp;\text{for high quality image}\-\gamma&amp;\text{else}\end{array}\right.\right.
$$</p>
<p>其中，$\lambda$ 和 $\gamma$ 是超参数。$\lambda$ 用于控制图像质量，$\gamma$ 是当生成的图像质量较低时对奖励函数施加的惩罚。</p>
<h6 id="那么如何判断图像质量是低还是高呢">那么如何判断图像质量是低还是高呢？
</h6><p>这里并不是设定一个阈值，来判断好坏。
而是给定一个 prompt，然后为 steps 集合 $\mathcal{S} ={10,20,30,40,50}$ 中的每一步生成一张图像；
然后看图像分数是否在这五张图像中排名前k，我们认为图像质量得分高(我们经验地将k设置为3)。</p>
<blockquote>
<p>我的理解，就是看前面较小的时间 t 生成的图像是否能够排到前三，如果可以就是具有优化空间；</p>
</blockquote>
<p>那么优化目标就是最大化期望奖励：
$$\max_\mathbf{w}{\mathcal{L}}=\mathbb{E}_{\mathbf{u}\sim\pi_f}R(\mathbf{u})$$</p>
<p>使用策略梯度算法来学习 步长选择网络 $f_s$ 的参数 $w$。期望的梯度推导如下：</p>
<p>$$\nabla_\mathbf{w}{\mathcal{L}}=\mathbb{E}\left[R(\mathbf{u})\nabla_\mathbf{w}\log\pi^f(\mathbf{u}\mid\mathbf{p})\right]$$</p>
<p>使用小批量样本,用蒙特卡罗采样进一步近似，得到：</p>
<p>$$\nabla_{\mathbf{w}}\mathcal{L}\approx\frac{1}{B}\sum_{i=1}^{B}\left[R\left(\mathbf{u}<em>{i}\right)\nabla</em>{\mathbf{w}}\log\pi^{f}\left(\mathbf{u}<em>{i}\mid\mathbf{p}</em>{i}\right)\right]$$</p>
<p>其中，$B$ 是批量大小，$\mathbf{u}_i$ 是从策略 $\pi^f$ 中采样的步长选择向量，$\mathbf{p}_i$ 是对应的文本提示。</p>
<h4 id="training-and-inference">Training and inference
</h4><ul>
<li>在上述训练过程之后，选择网络学习在推理时间和生成质量之间取得平衡的步长使用策略。</li>
<li>在推理阶段，对于不同的提示，使用s中的最大概率分数来确定生成步骤的数量，从而实现动态推理。</li>
</ul>
<blockquote>
<p>对于视频的生成，和上述的过程类似，只是在奖励函数上有所不同，这里不再赘述。</p>
</blockquote>
<hr>
<h1 id="experiments">Experiments
</h1><p>为了评估我们方法的有效性和通用性，我们在三个图像数据集上进行了广泛的实验:MS COCO 2017 ， Laion-COCO， DiffusionDB；
以及两个视频数据集:MSR-VTT[51]和InternVid[44]。</p>
<p>在MS COCO 2017中，我们的训练集由118287个文本描述组成，并且使用验证集中的所有25014个文本对进行测试。对于Laion-COCO，我们随机选择200K个文本描述用于训练，20K个文本图像对用于测试。DiffusionDB的训练集和测试集的划分遵循与Laico-COCO相同的范式。</p>
<h4 id="metric">Metric
</h4><p>FID 、 IS、 CLIP Score，NIQE， IQS</p>
<h4 id="results">Results
</h4><div align=center>
<img src="https://img.toyag.site/imgs/20231221134547.png" width = "50%" height = "" />
</div>
<blockquote>
<p>这里的 random 指的是：对不同提示使用随机步骤选择策略，我们报告5次运行使用的平均步骤。</p>
</blockquote>
<p>我们知道之前的扩散轨迹都是通过 ODE求解器来求解的，作者使用稳定扩散中的DPM-Solver[21]采样器来评估其在图像生成中的性能，如下表所示。与50步生成相比，AdaDiff平均为每个提示分配24.09步，进一步节省50.8% 的生成时间，同时保持相当的图像质量。
与随机策略相比，学习后的自适应策略不仅使推理时间提高了19.6%，而且在五个指标上提高了图像质量。这些结果表明，AdaDiff可以作为即插即用组件与其他加速方法相结合，用于动态生成。</p>
<div align=center>
<img src="https://img.toyag.site/imgs/20231221135228.png" width = "50%" height = "" />
</div>
<blockquote>
<p>实验来看，图像质量其实就是 0.0几  个点，这个说是相当。但是时间上的提升还是很明显的。
但是从这个角度来看，文本提示的丰富性难道只通过目标数量或者拥挤程度来衡量吗？从这个地方来加速，感觉有点不太合理。</p>
</blockquote>
<div align=center>
<img src="https://img.toyag.site/imgs/20231221135901.png" width = "40%" height = "" />
<img src="https://img.toyag.site/imgs/20231221135947.png" width = "40%" height = "" />
</div>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/diffusion-%E5%8A%A0%E9%80%9F/">Diffusion 加速</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    <section class="article-lastmod">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



        <span>
            Last updated on Aug 01, 2024 01:18 &#43;0800
        </span>
    </section></footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.body, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>
    
</article>

    

    

<aside class="related-content--wrapper">
    <h2 class="section-title">Related content</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="has-image">
    <a href="/p/cross-attention-makes-inference-cumbersome-in-text-to-image-diffusion-models/">
        
        
            <div class="article-image">
                
                    <img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202405061001750.png" loading="lazy" data-key="" data-hash="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202405061001750.png"/>
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models</h2>
        </div>
    </a>
</article>

            
                
<article class="has-image">
    <a href="/p/sdxs-real-time-one-step-latent-diffusion-models-with-image-conditions/">
        
        
            <div class="article-image">
                
                    <img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401795.png" loading="lazy" data-key="" data-hash="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401795.png"/>
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions</h2>
        </div>
    </a>
</article>

            
                
<article class="has-image">
    <a href="/p/faster-diffusion-rethinking-the-role-of-unet-encoder-in-diffusion-models/">
        
        
            <div class="article-image">
                <img src="/p/faster-diffusion-rethinking-the-role-of-unet-encoder-in-diffusion-models/assets/Faster%20Diffusion/image-7.39bc41021047cf57f10a328634f07859_hube457628ccb7476358da95c276ecf5b4_788663_250x150_fill_box_smart1_3.png" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models"
                        
                        data-hash="md5-ObxBAhBHz1fxCjKGNPB4WQ==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models</h2>
        </div>
    </a>
</article>

            
                
<article class="has-image">
    <a href="/p/all-papers-about-diffusion-models-on-arxiv/">
        
        
            <div class="article-image">
                <img src="/p/all-papers-about-diffusion-models-on-arxiv/_hu97d061de95afac90ca03bfd8d919a16b_239242_538f2624a81473eb1fd40013472e5733.jpeg" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post All papers about diffusion models on arxiv"
                        
                        data-hash="md5-ho7mJyGJc4P2VN3XPYZazA==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">All papers about diffusion models on arxiv</h2>
        </div>
    </a>
</article>

            
                
<article class="has-image">
    <a href="/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/">
        
        
            <div class="article-image">
                <img src="/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/assets/index/image.1fa74d1073fae62cdc25d8c51b2d6cda_hu5d9292eb672e6ca5e72e74d52b3afcdc_3182605_250x150_fill_box_smart1_3.png" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post StoryDiffusion：让 Diffusion 用来生成故事插画和视频"
                        
                        data-hash="md5-H6dNEHP65izcJdjFGy1s2g==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">StoryDiffusion：让 Diffusion 用来生成故事插画和视频</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    <div class="disqus-container">
    <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "hugo-theme-stack" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

<style>
    .disqus-container {
        background-color: var(--card-background);
        border-radius: var(--card-border-radius);
        box-shadow: var(--shadow-l1);
        padding: var(--card-padding);
    }
</style>

<script>
    window.addEventListener('onColorSchemeChange', (e) => {
        if (typeof DISQUS == 'object') {
            DISQUS.reset({
                reload: true
            });
        }
    })
</script>

    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2020 - 
        
        2024 Tlooh
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.26.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
