<!DOCTYPE html>
<html lang="en" dir="ltr">
    <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="StoryDiffusion：让 Diffusion 用来生成故事插画和视频 Motivation 目前生成主题一致（例如，具有一致身份和着装的角色）的图像和视频来描述整个故事是困难的。 像IP-Adapter这种，由于图像强语义的引导，导致文本的控制信号较弱； 而类似 InstantID 和 PhotoMaker 这种，关注身份可控性，但无法保证服装和场景的一致性。 Methods 主要分为两个阶段： First Stage: 模型使用 Consistent Self-Attention 以 training-free 的方式来生成主题一致的图像，这些图像可以直接用来讲故事，也可以用来作为第二阶段的输入； Second Stage: 模型利用这些主题一致的图像来生成一致的过渡视频； Training-free consistent images generation 由上图可以看到，第一阶段会把一段故事分成一个个的prompt，然后在一个batch里面去生成。这一部分的问题就是如何保证batch里面人物主体的一致性。 简单来说，这里做的一个操作就是提出了 CAB模块 ，然后将其插入到原始的自注意模块中，并重用原始自注意权重来达到无需训练和可插拔。 具体来说，给定一组图像特征 $\\mathcal{I}\\in\\mathbb{R}^{B\\times N\\times}$ ,其中$B, N, C$ 分别代表 batch size, number of tokens in each image, and channel num , 另外我们定义函数 $\\text{Attention}(X_k,X_q,X_v)$ 计算自注意。 $X_k,X_q,X_v$ 分别代表q,k,v, 原始自注意在$\\mathcal{I}$中的每个图像特征$I_i$内独立进行。特征 $I_i$ 会被投射到 $Q_i,K_i,\\tilde{V_i}$ 然后经过自注意函数： $$O_i=\\text{Attention}\\left(Q_i,K_i,V_i\\right).$$ 为了在Batch size中的图像之间建立交互以保持主题一致性，我们会从Batch中的其他图像特征中采样了一些 tokens: $$S_i=\\text{RandSample}\\left(I_1,I_2,..,I_{i-1},I_{i+1},&hellip;,I_{B-1},I_B\\right),$$ 采样后，我们将采样的标记$S_i$与图像特征$I_i$配对，形成一组新的标记$P_i$。然后将$P_i$进行投射成新的 K、V,原来的Q不变，最后，我们计算自我注意如下： $$O_i=\\text{Attention}\\left(Q_i,K_{Pi},V_{Pi}\\right).$$ 给定配对的标记，我们的方法跨一批图像执行自关注，促进不同图像特征之间的交互。这种类型的交互促进了模型在生成过程中趋同于人物、面孔和服装。">
<title>StoryDiffusion：让 Diffusion 用来生成故事插画和视频</title>

<link rel='canonical' href='http://localhost:1313/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/'>

<link rel="stylesheet" href="/scss/style.min.add8fb3bab9b743b892af012a73b6880459349cb1f020823ea8cdafee6e03d6d.css"><meta property='og:title' content="StoryDiffusion：让 Diffusion 用来生成故事插画和视频">
<meta property='og:description' content="StoryDiffusion：让 Diffusion 用来生成故事插画和视频 Motivation 目前生成主题一致（例如，具有一致身份和着装的角色）的图像和视频来描述整个故事是困难的。 像IP-Adapter这种，由于图像强语义的引导，导致文本的控制信号较弱； 而类似 InstantID 和 PhotoMaker 这种，关注身份可控性，但无法保证服装和场景的一致性。 Methods 主要分为两个阶段： First Stage: 模型使用 Consistent Self-Attention 以 training-free 的方式来生成主题一致的图像，这些图像可以直接用来讲故事，也可以用来作为第二阶段的输入； Second Stage: 模型利用这些主题一致的图像来生成一致的过渡视频； Training-free consistent images generation 由上图可以看到，第一阶段会把一段故事分成一个个的prompt，然后在一个batch里面去生成。这一部分的问题就是如何保证batch里面人物主体的一致性。 简单来说，这里做的一个操作就是提出了 CAB模块 ，然后将其插入到原始的自注意模块中，并重用原始自注意权重来达到无需训练和可插拔。 具体来说，给定一组图像特征 $\\mathcal{I}\\in\\mathbb{R}^{B\\times N\\times}$ ,其中$B, N, C$ 分别代表 batch size, number of tokens in each image, and channel num , 另外我们定义函数 $\\text{Attention}(X_k,X_q,X_v)$ 计算自注意。 $X_k,X_q,X_v$ 分别代表q,k,v, 原始自注意在$\\mathcal{I}$中的每个图像特征$I_i$内独立进行。特征 $I_i$ 会被投射到 $Q_i,K_i,\\tilde{V_i}$ 然后经过自注意函数： $$O_i=\\text{Attention}\\left(Q_i,K_i,V_i\\right).$$ 为了在Batch size中的图像之间建立交互以保持主题一致性，我们会从Batch中的其他图像特征中采样了一些 tokens: $$S_i=\\text{RandSample}\\left(I_1,I_2,..,I_{i-1},I_{i+1},&hellip;,I_{B-1},I_B\\right),$$ 采样后，我们将采样的标记$S_i$与图像特征$I_i$配对，形成一组新的标记$P_i$。然后将$P_i$进行投射成新的 K、V,原来的Q不变，最后，我们计算自我注意如下： $$O_i=\\text{Attention}\\left(Q_i,K_{Pi},V_{Pi}\\right).$$ 给定配对的标记，我们的方法跨一批图像执行自关注，促进不同图像特征之间的交互。这种类型的交互促进了模型在生成过程中趋同于人物、面孔和服装。">
<meta property='og:url' content='http://localhost:1313/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/'>
<meta property='og:site_name' content='Tlooh Blog Site'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='ID一致性' /><meta property='article:published_time' content='2024-07-19T01:12:44&#43;08:00'/><meta property='article:modified_time' content='2024-07-30T01:12:44&#43;08:00'/><meta property='og:image' content='http://localhost:1313/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/assets/index/image.png' />
<meta name="twitter:title" content="StoryDiffusion：让 Diffusion 用来生成故事插画和视频">
<meta name="twitter:description" content="StoryDiffusion：让 Diffusion 用来生成故事插画和视频 Motivation 目前生成主题一致（例如，具有一致身份和着装的角色）的图像和视频来描述整个故事是困难的。 像IP-Adapter这种，由于图像强语义的引导，导致文本的控制信号较弱； 而类似 InstantID 和 PhotoMaker 这种，关注身份可控性，但无法保证服装和场景的一致性。 Methods 主要分为两个阶段： First Stage: 模型使用 Consistent Self-Attention 以 training-free 的方式来生成主题一致的图像，这些图像可以直接用来讲故事，也可以用来作为第二阶段的输入； Second Stage: 模型利用这些主题一致的图像来生成一致的过渡视频； Training-free consistent images generation 由上图可以看到，第一阶段会把一段故事分成一个个的prompt，然后在一个batch里面去生成。这一部分的问题就是如何保证batch里面人物主体的一致性。 简单来说，这里做的一个操作就是提出了 CAB模块 ，然后将其插入到原始的自注意模块中，并重用原始自注意权重来达到无需训练和可插拔。 具体来说，给定一组图像特征 $\\mathcal{I}\\in\\mathbb{R}^{B\\times N\\times}$ ,其中$B, N, C$ 分别代表 batch size, number of tokens in each image, and channel num , 另外我们定义函数 $\\text{Attention}(X_k,X_q,X_v)$ 计算自注意。 $X_k,X_q,X_v$ 分别代表q,k,v, 原始自注意在$\\mathcal{I}$中的每个图像特征$I_i$内独立进行。特征 $I_i$ 会被投射到 $Q_i,K_i,\\tilde{V_i}$ 然后经过自注意函数： $$O_i=\\text{Attention}\\left(Q_i,K_i,V_i\\right).$$ 为了在Batch size中的图像之间建立交互以保持主题一致性，我们会从Batch中的其他图像特征中采样了一些 tokens: $$S_i=\\text{RandSample}\\left(I_1,I_2,..,I_{i-1},I_{i+1},&hellip;,I_{B-1},I_B\\right),$$ 采样后，我们将采样的标记$S_i$与图像特征$I_i$配对，形成一组新的标记$P_i$。然后将$P_i$进行投射成新的 K、V,原来的Q不变，最后，我们计算自我注意如下： $$O_i=\\text{Attention}\\left(Q_i,K_{Pi},V_{Pi}\\right).$$ 给定配对的标记，我们的方法跨一批图像执行自关注，促进不同图像特征之间的交互。这种类型的交互促进了模型在生成过程中趋同于人物、面孔和服装。"><meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content='http://localhost:1313/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/assets/index/image.png' />
  


    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended">



        <div id="article-toolbar" style="position: sticky;top: 5px;z-index: 1000;">
            <a href="/" class="back-home">
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="15 6 9 12 15 18" />
</svg>



                <span>Back</span>
            </a>
        </div>
    

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">Table of contents</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#motivation">Motivation</a></li>
    <li><a href="#methods">Methods</a>
      <ol>
        <li><a href="#training-free-consistent-images-generation">Training-free consistent images generation</a></li>
      </ol>
    </li>
    <li><a href="#semantic-motion-predictor-for-video-generation">Semantic motion predictor for video generation</a></li>
    <li><a href="#实验">实验</a></li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="has-image main-article">
    <header class="article-header">
        <div class="article-image">
            <a href="/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/">
                <img src="/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/assets/index/image_hu5d9292eb672e6ca5e72e74d52b3afcdc_3182605_800x0_resize_box_3.png"
                        srcset="/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/assets/index/image_hu5d9292eb672e6ca5e72e74d52b3afcdc_3182605_800x0_resize_box_3.png 800w, /p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/assets/index/image_hu5d9292eb672e6ca5e72e74d52b3afcdc_3182605_1600x0_resize_box_3.png 1600w"
                        width="800" 
                        height="479" 
                        loading="lazy"
                        alt="Featured image of post StoryDiffusion：让 Diffusion 用来生成故事插画和视频" />
                
            </a>
        </div>
    

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/diffusion/" >
                Diffusion
            </a>
        
            <a href="/categories/papers/" >
                Papers
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/">StoryDiffusion：让 Diffusion 用来生成故事插画和视频</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Jul 19, 2024</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    1 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <h1 id="storydiffusion让-diffusion-用来生成故事插画和视频">StoryDiffusion：让 Diffusion 用来生成故事插画和视频
</h1><hr>
<h2 id="motivation">Motivation
</h2><p>目前生成主题一致（例如，具有一致身份和着装的角色）的图像和视频来描述整个故事是困难的。</p>
<ul>
<li>像IP-Adapter这种，由于图像强语义的引导，导致文本的控制信号较弱；</li>
<li>而类似 InstantID 和 PhotoMaker 这种，关注身份可控性，但无法保证服装和场景的一致性。</li>
</ul>
<h2 id="methods">Methods
</h2><p>主要分为两个阶段：</p>
<ul>
<li><code>First Stage</code>: 模型使用 <code>Consistent Self-Attention</code> 以 training-free 的方式来生成主题一致的图像，这些图像可以直接用来讲故事，也可以用来作为第二阶段的输入；</li>
<li><code>Second Stage</code>: 模型利用这些主题一致的图像来生成一致的过渡视频；</li>
</ul>
<p><img src="/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/image.png"
	width="1750"
	height="916"
	srcset="/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/image_hudef821d89c4aa164073f78c294353610_1337627_480x0_resize_box_3.png 480w, /p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/image_hudef821d89c4aa164073f78c294353610_1337627_1024x0_resize_box_3.png 1024w"
	loading="lazy"
	
		alt="Alt text"
	
	
		class="gallery-image" 
		data-flex-grow="191"
		data-flex-basis="458px"
	
></p>
<h3 id="training-free-consistent-images-generation">Training-free consistent images generation
</h3><p>由上图可以看到，第一阶段会把一段故事分成一个个的prompt，然后在一个batch里面去生成。这一部分的问题就是如何保证batch里面人物主体的一致性。</p>
<p>简单来说，这里做的一个操作就是提出了 <strong>CAB模块</strong> ，然后将其插入到原始的自注意模块中，并重用原始自注意权重来达到无需训练和可插拔。</p>
<p>具体来说，给定一组图像特征 $\mathcal{I}\in\mathbb{R}^{B\times N\times}$ ,其中$B, N, C$ 分别代表 batch size, number of tokens in each image, and channel num , 另外我们定义函数 $\text{Attention}(X_k,X_q,X_v)$ 计算自注意。 $X_k,X_q,X_v$ 分别代表q,k,v, 原始自注意在$\mathcal{I}$中的每个图像特征$I_i$内独立进行。特征 $I_i$ 会被投射到 $Q_i,K_i,\tilde{V_i}$ 然后经过自注意函数：</p>
<p>$$O_i=\text{Attention}\left(Q_i,K_i,V_i\right).$$</p>
<p>为了在Batch size中的图像之间建立交互以保持主题一致性，我们会从Batch中的其他图像特征中采样了一些 tokens:</p>
<p>$$S_i=\text{RandSample}\left(I_1,I_2,..,I_{i-1},I_{i+1},&hellip;,I_{B-1},I_B\right),$$</p>
<p>采样后，我们将采样的标记$S_i$与图像特征$I_i$配对，形成一组新的标记$P_i$。然后将$P_i$进行投射成新的 K、V,原来的Q不变，最后，我们计算自我注意如下：</p>
<p>$$O_i=\text{Attention}\left(Q_i,K_{Pi},V_{Pi}\right).$$</p>
<p>给定配对的标记，我们的方法跨一批图像执行自关注，促进不同图像特征之间的交互。这种类型的交互促进了模型在生成过程中趋同于人物、面孔和服装。</p>
<p><img src="/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/image-1.png"
	width="1748"
	height="964"
	srcset="/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/image-1_huc82d7b839fc53b7a5c86bd3cdfedcac6_561260_480x0_resize_box_3.png 480w, /p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/image-1_huc82d7b839fc53b7a5c86bd3cdfedcac6_561260_1024x0_resize_box_3.png 1024w"
	loading="lazy"
	
		alt="Alt text"
	
	
		class="gallery-image" 
		data-flex-grow="181"
		data-flex-basis="435px"
	
></p>
<h2 id="semantic-motion-predictor-for-video-generation">Semantic motion predictor for video generation
</h2><p>通过在每对相邻图像之间插入帧，可以将生成的字符一致图像序列进一步细化为视频。这可以看作是一个<strong>以已知的开始帧和结束帧为条件的视频生成任务。</strong></p>
<p>提出了一个 Semantic Motion Predictor, 用以将图像编码到语意空间来捕获空间信息。具体来说，我们设计了一个函数$E$来建立RGB图像到语意空间的映射，而不是直接使用线性层进行投射。（说了这么多，就使用CLIP image encoder来作为 E）
<img src="/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/image-2.png"
	width="378"
	height="80"
	srcset="/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/image-2_huc82d7b839fc53b7a5c86bd3cdfedcac6_13009_480x0_resize_box_3.png 480w, /p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/image-2_huc82d7b839fc53b7a5c86bd3cdfedcac6_13009_1024x0_resize_box_3.png 1024w"
	loading="lazy"
	
		alt="Alt text"
	
	
		class="gallery-image" 
		data-flex-grow="472"
		data-flex-basis="1134px"
	
></p>
<p>然后，训练了一个 <code>transformer-based structure predictor</code> 来预测每个中间帧，预测器首先进行线性插值，将两个帧 $K_s$ 和 $K_e$ 扩展成序列 $K_1, K_2, \ldots, K_L$，其中 $L$ 是所需的视频长度。然后，将序列 $K_1, K_2, \ldots, K_L$ 输入到一系列transformer块 $B$ 中，以预测过渡帧：</p>
<p>$$
P_1, P_2, \ldots, P_l = B(K_1, K_2, \ldots, K_L)。
$$</p>
<p>接下来，将这些图像语义嵌入 $P_1, P_2, \ldots, P_L$ 作为控制信号，并将视频扩散模型作为解码器，以利用视频扩散模型的生成能力。我们还插入了额外的线性层，将这些嵌入投影到键和值，涉及到U-Net的交叉注意力。形式上，在扩散过程中，对于每个视频帧特征 $V_i$，我们将文本嵌入 $T$ 和预测的图像语义嵌入 $P_i$ 连接起来。交叉注意力计算如下：</p>
<p>$$
V_i = \text{CrossAttention}(V_i, \text{concat}(T, P_i), \text{concat}(T, P_i))。
$$</p>
<p>类似于以前的视频生成方法，我们通过计算预测过渡视频 $O = (O_1, O_2, \ldots, O_L)$ 和真实帧 $G = (G_1, G_2, \ldots, G_L)$ 之间的均方误差损失（MSE Loss）来优化我们的模型：</p>
<p>$$
\text{Loss} = \text{MSE}(G, O)。
$$</p>
<p>通过将图像编码到图像语义空间以整合空间位置关系，我们的语义运动预测器可以更好地建模运动信息，从而生成具有大运动的平滑过渡视频。结果和比较可以在图1和图5中观察到，显示了显著的改进。</p>
<h2 id="实验">实验
</h2><ul>
<li>图像生成之间的对比</li>
</ul>
<p><img src="/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/image-3.png"
	width="1270"
	height="882"
	srcset="/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/image-3_huc82d7b839fc53b7a5c86bd3cdfedcac6_1428272_480x0_resize_box_3.png 480w, /p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/image-3_huc82d7b839fc53b7a5c86bd3cdfedcac6_1428272_1024x0_resize_box_3.png 1024w"
	loading="lazy"
	
		alt="Alt text"
	
	
		class="gallery-image" 
		data-flex-grow="143"
		data-flex-basis="345px"
	
></p>
<p><img src="/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/image-4.png"
	width="1278"
	height="818"
	srcset="/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/image-4_huc82d7b839fc53b7a5c86bd3cdfedcac6_1240991_480x0_resize_box_3.png 480w, /p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/image-4_huc82d7b839fc53b7a5c86bd3cdfedcac6_1240991_1024x0_resize_box_3.png 1024w"
	loading="lazy"
	
		alt="Alt text"
	
	
		class="gallery-image" 
		data-flex-grow="156"
		data-flex-basis="374px"
	
></p>
<ul>
<li>定量比较</li>
</ul>
<p><img src="/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/image-5.png"
	width="1308"
	height="534"
	srcset="/p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/image-5_huc82d7b839fc53b7a5c86bd3cdfedcac6_298320_480x0_resize_box_3.png 480w, /p/storydiffusion%E8%AE%A9-diffusion-%E7%94%A8%E6%9D%A5%E7%94%9F%E6%88%90%E6%95%85%E4%BA%8B%E6%8F%92%E7%94%BB%E5%92%8C%E8%A7%86%E9%A2%91/image-5_huc82d7b839fc53b7a5c86bd3cdfedcac6_298320_1024x0_resize_box_3.png 1024w"
	loading="lazy"
	
		alt="Alt text"
	
	
		class="gallery-image" 
		data-flex-grow="244"
		data-flex-basis="587px"
	
></p>
<p>Video就不看了……</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/id%E4%B8%80%E8%87%B4%E6%80%A7/">ID一致性</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    <section class="article-lastmod">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



        <span>
            Last updated on Jul 30, 2024 01:12 CST
        </span>
    </section></footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.body, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>
    
</article>

    

    

<aside class="related-content--wrapper">
    <h2 class="section-title">Related content</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="has-image">
    <a href="/p/all-papers-about-diffusion-models-on-arxiv/">
        
        
            <div class="article-image">
                <img src="/p/all-papers-about-diffusion-models-on-arxiv/_hu97d061de95afac90ca03bfd8d919a16b_239242_538f2624a81473eb1fd40013472e5733.jpeg" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post All papers about diffusion models on arxiv"
                        
                        data-hash="md5-ho7mJyGJc4P2VN3XPYZazA==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">All papers about diffusion models on arxiv</h2>
        </div>
    </a>
</article>

            
                
<article class="has-image">
    <a href="/p/cross-attention-makes-inference-cumbersome-in-text-to-image-diffusion-models/">
        
        
            <div class="article-image">
                
                    <img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202405061001750.png" loading="lazy" data-key="" data-hash="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202405061001750.png"/>
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models</h2>
        </div>
    </a>
</article>

            
                
<article class="has-image">
    <a href="/p/sdxs-real-time-one-step-latent-diffusion-models-with-image-conditions/">
        
        
            <div class="article-image">
                
                    <img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401795.png" loading="lazy" data-key="" data-hash="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/202404281401795.png"/>
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions</h2>
        </div>
    </a>
</article>

            
                
<article class="has-image">
    <a href="/p/comat-aligning-text-to-image-diffusion-model-with-image-to-text-concept-matching/">
        
        
            <div class="article-image">
                
                    <img src="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/image-2.png" loading="lazy" data-key="" data-hash="https://raw.githubusercontent.com/Tlooh/blog-imgs/main/web/image-2.png"/>
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching</h2>
        </div>
    </a>
</article>

            
                
<article class="has-image">
    <a href="/p/faster-diffusion-rethinking-the-role-of-unet-encoder-in-diffusion-models/">
        
        
            <div class="article-image">
                <img src="/p/faster-diffusion-rethinking-the-role-of-unet-encoder-in-diffusion-models/assets/Faster%20Diffusion/image-7.39bc41021047cf57f10a328634f07859_hube457628ccb7476358da95c276ecf5b4_788663_250x150_fill_box_smart1_3.png" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models"
                        
                        data-hash="md5-ObxBAhBHz1fxCjKGNPB4WQ==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    <div class="disqus-container">
    <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "hugo-theme-stack" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

<style>
    .disqus-container {
        background-color: var(--card-background);
        border-radius: var(--card-border-radius);
        box-shadow: var(--shadow-l1);
        padding: var(--card-padding);
    }
</style>

<script>
    window.addEventListener('onColorSchemeChange', (e) => {
        if (typeof DISQUS == 'object') {
            DISQUS.reset({
                reload: true
            });
        }
    })
</script>

    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2020 - 
        
        2024 Tlooh
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.26.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
